---
title: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, error=F, warning=F)
library(tidyverse) # CSV file I/O, e.g. the read_csv function
library(RColorBrewer)
library(viridis)
library(plotly) #contain hex to RGB conversion
#date
library(lubridate)
#special
library(packcircles)
#text
library(tidytext)
library(quanteda)
library(udpipe)
#animate
library(gganimate)
#theme
my_theme <- function(base_size = 12, base_family = "Helvetica"){
    theme_minimal() +
    theme(axis.title.y = element_blank(),axis.title.x = element_blank(),
    plot.title = element_text(face="bold", size=16),
    axis.text = element_text(face="bold"),
    plot.background = element_rect(fill = 'ghostwhite',color='white'),
    legend.position = 'None', legend.title = element_blank())
}

```

#Section 1: input
## Load data
```{r input}
havana=readLines('/Users/hannah/git_repo/opendata_viz/lyric_repetitiveness/havana.txt') 
#lowercase everything
df = data.frame(line = 1:71, txt = havana)%>% 
#remove bracket
  mutate(txt_original = as.character(tolower(txt)),
         txt=str_replace_all(txt_original, '\\(.*?\\)', ''))
```

# Section 2: Text processing
```{r collocationmethod for reference}
ud_model <- udpipe_download_model(language = "english")
ud_model <- udpipe_load_model(ud_model$file_model)

x <- udpipe_annotate(ud_model, x = as.character(df$txt), doc_id = df$line)
x <- as.data.frame(x) 

colloc <- keywords_collocation(x, term = "token", group = c("doc_id", "sentence_id"),
ngram_max = 6, n_min = 2)
```

step 1: find common phrases
```{r}
topfeatures(dfm(df$txt, ngrams = c(2,5), verbose = FALSE), n=30)
df%>%filter(grepl("ooh.na-na-na",txt))%>%count(1)

phraseHavana = c("my heart is in havana",
"ooh.na-na",
"east atlanta",
"took me back")

#occurence of these phrases
cnt <- function (i) {
  return(data.frame(n = sum(str_count(df$txt, i))))
}
df_phrase <- phraseHavana %>%map_dfr(cnt) %>% cbind(phraseHavana) %>%select(word = phraseHavana, n)
```

step 2: tokenize the rest
```{r}
toreplace=paste(phraseHavana,collapse="|")
df_word <- df %>% 
  mutate(txt = str_replace_all(txt_original, toreplace, ""))%>%
  unnest_tokens(word, txt)

df_word_digest <- df_word %>%
  group_by(word) %>%
  summarize(n = n()) 
```

step 3: re-integrate
```{r}
df_combined <- df_word_digest %>%
  rbind(df_phrase)
```




# Section 3: Visualization
## calc circle packing layout, return a list of dataframe
```{r fun}
circle_layout <- function(df, n) {
  packing <- circleProgressiveLayout(df$n, sizetype='area')
  #leave gaps between circles
  packing$radius=0.8*packing$radius
  # out <- list()
  #data contains coordinate/radius of each circle
  data = cbind(df, packing) %>% mutate(id=1:n())
  #dat.gg layout each vertice on the polygon which is a proxy for circle
  dat.gg <- circleLayoutVertices(packing, npoints=50)
  # out$packing = packing
  # out$data = data
  # out$dat.gg = dat.gg
  circles = dat.gg %>% left_join(data[c("id","word")], by="id")
  return(circles)
}
```



```{r}
show_digest <- function(df) {
  df_dummy <- df %>% 
  mutate(n = 1)
  
  x <- circle_layout(df_dummy, n) %>% 
  mutate(state = as.factor("without repetition"))
  y <- circle_layout(df, n) %>% 
  mutate(state = as.factor("with repetition"))

  z <- x %>%
  rbind(y)
  
  return(z)
}
```


```{r}
p <- show_digest(df_combined) %>%
  ggplot() + 
  geom_polygon(aes(x, y, fill=id, 
                   text =  paste0("word: ",word)
                   ),  alpha = 0.8) +
  scale_fill_viridis() +
  #geom_text(data = data, aes(x, y, size=value, label = group), color="black") +
  theme_void() + 
  theme(legend.position="none")+ 
  coord_equal() + facet_grid(.~state)

ggplotly(p)
```




